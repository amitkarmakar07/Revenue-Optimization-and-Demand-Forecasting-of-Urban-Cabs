# -*- coding: utf-8 -*-
"""urbancab_model _training

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/166DZUL8fsPlPb44KdqJciN0rBWOOf4_m
"""

!pip install duckdb --quiet

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
# %load_ext duckdb
import duckdb
import matplotlib.pyplot as plt
import os
import seaborn as sns
from scipy import stats
from scipy.stats import shapiro, anderson, jarque_bera, poisson, norm , randint
from statsmodels.stats.outliers_influence import variance_inflation_factor
from scipy.stats import f_oneway, ttest_ind, chi2_contingency
import numpy
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import train_test_split , RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
from xgboost import XGBRegressor
from scipy.stats import uniform
from sklearn.ensemble import GradientBoostingRegressor
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

"""## Loading the data"""

trip_df = pd.read_csv('/content/drive/MyDrive/Yellow_Taxi_Trip_Data.csv')
zone_df = pd.read_csv('/content/drive/MyDrive/taxi_zone_lookup.csv')

trip_df.head()

zone_df.head()

trip_df.info()

trip_df.describe()

"""## Handling Missing Values"""

trip_df.isnull().sum()

trip_df.dropna(subset=['VendorID', 'RatecodeID', 'store_and_fwd_flag'], inplace=True)

trip_df['payment_type'].fillna(trip_df['payment_type'].mode()[0], inplace=True)

mean_passenger_count = abs(trip_df['passenger_count'].mean())
trip_df['passenger_count'].fillna(round(mean_passenger_count), inplace=True)

zone_df.isnull().sum()

zone_df.dropna(inplace=True)

"""## Handling Duplicates"""

duplicate_count = trip_df.duplicated().sum()
duplicate_count

zone_df.duplicated().sum()

"""### Remove Unnessecary Columns"""

# List of important columns to keep
required_columns = [
    'tpep_pickup_datetime', 'tpep_dropoff_datetime',
    'passenger_count', 'trip_distance',
    'PULocationID', 'DOLocationID',
    'payment_type', 'fare_amount', 'tip_amount',
    'tolls_amount', 'total_amount', 'congestion_surcharge'
    ]

# Keep only those columns
taxi_df = trip_df[required_columns]

# Keep only 'LocationID' and 'Borough' columns
zone_df = zone_df[['LocationID', 'Zone']]
zone_df.shape

taxi_df.to_csv('/content/drive/MyDrive/Taxi_Data_Analysis/cleaned_taxi.csv', index=False)
zone_df.to_csv('/content/drive/MyDrive/Taxi_Data_Analysis/cleaned_zone.csv', index=False)

"""## Merging Table using DuckDB"""

duckdb.sql("""
    CREATE TABLE trip AS
    SELECT *
    FROM read_csv_auto('/content/drive/MyDrive/Taxi_Data_Analysis/cleaned_taxi.csv')
    """)

duckdb.sql("""
    CREATE TABLE zone AS
    SELECT *
    FROM read_csv_auto('/content/drive/MyDrive/Taxi_Data_Analysis/cleaned_zone.csv')
    """)

duckdb.sql("SELECT * FROM trip LIMIT 5").df()

duckdb.sql("""
    CREATE TABLE trips_with_pickup AS
    SELECT t.*, z.Zone AS PU_Zone
    FROM trip t
    LEFT JOIN zone z
    ON t.PULocationID = z.LocationID
  """)

duckdb.sql("""
    CREATE TABLE trips_with_zones AS
    SELECT t.*, z.Zone AS DO_Zone
    FROM trips_with_pickup t
    LEFT JOIN zone z
    ON t.DOLocationID = z.LocationID
    """)

merged_df = duckdb.sql("SELECT * FROM trips_with_zones").df()

duckdb.sql("select * from merged_df limit 5").df()

"""## Feature Extraction

### Datetime Feature Extraction
"""

# convert to datetime
merged_df['tpep_pickup_datetime'] = pd.to_datetime(merged_df['tpep_pickup_datetime'])
merged_df['tpep_dropoff_datetime'] = pd.to_datetime(merged_df['tpep_dropoff_datetime'])

merged_df['pickup_hour'] = merged_df['tpep_pickup_datetime'].dt.hour
merged_df['pickup_day_of_week'] = merged_df['tpep_pickup_datetime'].dt.dayofweek
merged_df['is_weekend'] = merged_df['pickup_day_of_week'].isin([5, 6]).astype(int)
merged_df['trip_duration_minutes'] = (merged_df['tpep_dropoff_datetime'] - merged_df['tpep_pickup_datetime']).dt.total_seconds() / 60

# Rush hour flags (urban)
merged_df['is_rush_hour'] = merged_df['pickup_hour'].isin( [15, 16, 17,  18, 19]).astype(int)

"""### Trip-level Calculations"""

# Avoid divide-by-zero
merged_df['fare_per_km'] = merged_df['fare_amount'] / merged_df['trip_distance'].replace(0, 0.1)
merged_df['tip_percent'] = (merged_df['tip_amount'] / merged_df['fare_amount'].replace(0, 0.1)) * 100

"""### Trip Route"""

merged_df['trip_route'] = merged_df['PU_Zone'] + " to " + merged_df['DO_Zone']

"""### Flag Suspicious or Low-quality Trips"""

merged_df['is_zero_fare'] = (merged_df['fare_amount'] == 0).astype(int)
merged_df['is_short_trip'] = (merged_df['trip_distance'] < 0.5).astype(int)

important_cols = [
    'tpep_pickup_datetime',
    'pickup_hour',
    'pickup_day_of_week',
    'is_weekend',
    'trip_duration_minutes',
    'trip_distance',
    'fare_amount',
    'tip_amount',
    'total_amount',
    'PU_Zone',
    'DO_Zone',
    'passenger_count',
    'fare_per_km',
    'tip_percent',
    'is_rush_hour',
    'trip_route'
    ]

cleaned_df = merged_df[important_cols].copy()

cleaned_df.to_csv('/content/drive/MyDrive/Taxi_Data_Analysis/cleaned_df.csv', index=False)

df= pd.read_csv('/content/drive/MyDrive/Taxi_Data_Analysis/cleaned_df.csv')

"""## EDA using DuckDB"""

duckdb.sql("""
    CREATE TABLE trip_data AS
    SELECT *
    FROM read_csv_auto('/content/drive/MyDrive/Taxi_Data_Analysis/cleaned_df.csv')
    """)

### Trip Demand & Revenue by Hour

df_busy_hours = duckdb.sql("""
SELECT
Â  pickup_hour,
Â  COUNT(*) AS total_trips,
Â  SUM(total_amount) AS total_revenue
FROM trip_data
GROUP BY pickup_hour
ORDER BY pickup_hour
""").df()

fig, ax1 = plt.subplots(figsize=(8, 4))

ax1.bar(df_busy_hours['pickup_hour'], df_busy_hours['total_trips'], color='#E66C37', label='Total Trips')
ax1.set_ylabel('Total Trips', color='black')
ax1.tick_params(axis='y', labelcolor='black')


ax2 = ax1.twinx()
ax2.plot(df_busy_hours['pickup_hour'], df_busy_hours['total_revenue'], color='black', marker='o', label='Total Revenue')
ax2.set_ylabel('Total Revenue ($)', color='black')
ax2.tick_params(axis='y', labelcolor='black')


plt.title('Hourly Trip Demand & Revenue')
ax1.set_xlabel('Pickup Hour (0â€“23)')
fig.tight_layout()
plt.show()

# Number of trips by day

df_days = duckdb.sql("""
SELECT
    pickup_day_of_week,
    is_weekend,
    COUNT(*) AS total_trips
    FROM trip_data
    GROUP BY pickup_day_of_week, is_weekend
    ORDER BY pickup_day_of_week
    """).df()

plt.figure(figsize=(8, 4))
plt.plot(df_days['pickup_day_of_week'], df_days['total_trips'],
         color='#E66C37', marker='o', linewidth=2, markersize=8)

plt.title("Trips by Day of Week", fontsize=14)
plt.xlabel("Day of Week (0 = Monday, 6 = Sunday)", fontsize=12)
plt.ylabel("Total Trips", fontsize=12)
plt.xticks(df_days['pickup_day_of_week'])
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.gca().spines[['top', 'right']].set_visible(False)

plt.tight_layout()
plt.show()

###  Hourly patterns by day of week
# this help for identify in which day which time most trip , help in proper design reduce the idle
duckdb.sql(
    """SELECT pickup_hour,
            pickup_day_of_week,
                COUNT(*) as trip_count,
                    AVG(total_amount) as avg_revenue
                    FROM trip_data
                    GROUP BY pickup_hour, pickup_day_of_week
                    ORDER BY trip_count desc limit 7
                    """
).df()

# supply demand gap analysis

duckdb.sql("""
    -- Peak hour zone imbalances (6-8 PM)
    WITH pickups AS (
        SELECT PU_Zone AS zone, pickup_hour, COUNT(*) AS pickup_count
        FROM trip_data
        WHERE pickup_hour BETWEEN 18 AND 20
        GROUP BY PU_Zone, pickup_hour
    ),
    dropoffs AS (
        SELECT DO_Zone AS zone, pickup_hour, COUNT(*) AS dropoff_count
        FROM trip_data
        WHERE pickup_hour BETWEEN 18 AND 20
        GROUP BY DO_Zone, pickup_hour
    ),
    zone_net_demand AS (
        SELECT
            COALESCE(p.zone, d.zone) AS zone,
            COALESCE(p.pickup_hour, d.pickup_hour) AS hour,
            COALESCE(p.pickup_count, 0) AS pickups,
            COALESCE(d.dropoff_count, 0) AS dropoffs,
            COALESCE(p.pickup_count, 0) - COALESCE(d.dropoff_count, 0) AS net_demand,
            CASE
                WHEN COALESCE(p.pickup_count, 0) > COALESCE(d.dropoff_count, 0)
                THEN 'UNDERSUPPLIED'
                ELSE 'OVERSUPPLIED'
            END AS zone_status
        FROM pickups p
        FULL OUTER JOIN dropoffs d
        ON p.zone = d.zone AND p.pickup_hour = d.pickup_hour
    )
    SELECT *
    FROM zone_net_demand
    ORDER BY net_demand DESC;
    """).df()

#spatial analysis
# Most common pickup-drop pairs (OD matrix)
top_routes_df = duckdb.sql("""
    SELECT
    trip_route,
     COUNT(*) AS trip_count
    FROM trip_data
    WHERE trip_route IS NOT NULL
    GROUP BY trip_route
    ORDER BY trip_count DESC
    LIMIT 5
""").df()

plt.figure(figsize=(8, 4))
sns.barplot(data=top_routes_df, y='trip_route', x='trip_count', palette='YlOrBr')
plt.title('Top 10 Most Frequent Pickup-Drop Routes')
plt.xlabel('Number of Trips')
plt.ylabel('Pickup â†’ Drop Route')
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.gca().spines[['top', 'right']].set_visible(False)
plt.tight_layout()
plt.show()

# Aggregating fare, tip, and duration by pickup zone
zone_profit_df = duckdb.sql("""
    SELECT
            PU_Zone,
            COUNT(*) AS total_trips,
            ROUND(AVG(fare_amount), 2) AS avg_fare,
            ROUND(AVG(tip_amount), 2) AS avg_tip,
            ROUND(AVG(trip_duration_minutes), 2) AS avg_duration
            FROM trip_data
            WHERE PU_Zone IS NOT NULL
            GROUP BY PU_Zone
            HAVING total_trips >= 100
            ORDER BY avg_fare DESC
            LIMIT 5
            """).df()

# Plot average fare and tip by PU_Zone
plt.figure(figsize=(10, 5))

# Avg Fare
plt.subplot(1, 2, 1)
sns.barplot(data=zone_profit_df, x='avg_fare', y='PU_Zone', palette='YlOrBr')
plt.title('Top 10 Zones by Avg Fare')
plt.xlabel('Average Fare ($)')
plt.ylabel('Pickup Zone')
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.gca().spines[['top', 'right']].set_visible(False)

# Avg Tip
plt.subplot(1, 2, 2)
sns.barplot(data=zone_profit_df, x='avg_tip', y='PU_Zone', palette='YlOrBr')
plt.title('Top 10 Zones by Avg Tip')
plt.xlabel('Average Tip ($)')
plt.ylabel('')
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.gca().spines[['top', 'right']].set_visible(False)

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

df = duckdb.sql("""
    SELECT PU_Zone, SUM(total_amount) AS total_revenue
    FROM trip_data
    GROUP BY PU_Zone
    ORDER BY total_revenue DESC
    LIMIT 10
    """).df()

plt.figure(figsize=(8,4))
plt.scatter(df['total_revenue'], df['PU_Zone'], color='#E66C37')
plt.xlabel("Total Revenue")
plt.title("Top 10 Pickup Zones by Revenue")
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

###Profitability: Tip % vs Fare per Km
# See if longer, higher fare trips also bring better tips â†’ helps in revenue optimization.

duckdb.sql("""
SELECT
    fare_per_km,
    tip_percent
    FROM trip_data
    WHERE fare_per_km BETWEEN 1 AND 10 AND tip_percent BETWEEN 0 AND 100
    """).df().plot.scatter(x='fare_per_km', y='tip_percent', alpha=0.5, color="#E66C37", title='Tip % vs Fare per Km')

# revenue vs passenger count
duckdb.sql("""
SELECT
     passenger_count,
      COUNT(*) AS trip_count,
      AVG(total_amount) AS avg_total_amount,
        SUM(total_amount) AS total_revenue
        FROM trip_data
        GROUP BY passenger_count
        ORDER BY avg_total_amount desc limit 5
        """).df()

duckdb.sql("""
    SELECT trip_distance, fare_per_km
        FROM trip_data
            WHERE trip_distance BETWEEN 1 AND 30 AND fare_per_km BETWEEN 1 AND 20
            """).df().plot.scatter(x='trip_distance',y='fare_per_km',
                         alpha=0.5,color='#E66C37',
                         title='Trip Distance vs Fare per Km',
                         grid=True)

"""## Statistical Analysis"""

trip_df=duckdb.sql("select * from trip_data").df();

"""###  Correlation Analysis & Multi-collinearity Check"""

numerical_cols = ['trip_distance', 'fare_amount', 'tip_amount', 'total_amount',
                  'passenger_count', 'fare_per_km', 'tip_percent', 'trip_duration_minutes',
                  'pickup_hour', 'pickup_day_of_week']
correlation_matrix = trip_df[numerical_cols].corr()
plt.figure(figsize=(8,6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
            square=True, fmt='.2f')
plt.title('Correlation Matrix of Numerical Features')
plt.tight_layout()
plt.show()

# Find highly correlated pairs (above 0.7 or below -0.7)
high_corr_pairs = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        corr_value = correlation_matrix.iloc[i, j]
        if abs(corr_value) > 0.7:
            high_corr_pairs.append({
                'Feature1': correlation_matrix.columns[i],
                'Feature2': correlation_matrix.columns[j],
                'Correlation': corr_value
            })

print("Highly correlated feature pairs:")
for pair in high_corr_pairs:
    print(f"{pair['Feature1']} vs {pair['Feature2']}: {pair['Correlation']:.3f}")

# Check for multicollinearity using VIF
vif_data = trip_df[numerical_cols].dropna()

# Calculate VIF for each feature
vif_results = []
for i, col in enumerate(vif_data.columns):
    vif_value = variance_inflation_factor(vif_data.values, i)
    vif_results.append({'Feature': col, 'VIF': vif_value})

vif_df = pd.DataFrame(vif_results)
print("\nVariance Inflation Factor (VIF):")
print("VIF > 10 indicates high multicollinearity")
print(vif_df.sort_values('VIF', ascending=False))

features_to_remove_for_modeling = ['total_amount', 'fare_amount']
features_to_keep_for_modeling = [
    'trip_distance', 'tip_amount', 'fare_per_km', 'tip_percent',
     'trip_duration_minutes', 'pickup_hour', 'pickup_day_of_week',
    'passenger_count'
]

"""###  Statistical Significance Testing (ANOVA/T-tests)"""

# T-test 1 -is weekend demand significantly different from weekday?
print("="*50)
print("TEST 1: Weekend vs Weekday Demand")
print("="*50)

# separate weekend and weekday data
weekend_data = trip_df[trip_df['is_weekend'] == 1]['total_amount']
weekday_data = trip_df[trip_df['is_weekend'] == 0]['total_amount']

# perform t-test
t_stat, p_value = ttest_ind(weekend_data, weekday_data)

print(f"Weekend trips average revenue: ${weekend_data.mean():.2f}")
print(f"Weekday trips average revenue: ${weekday_data.mean():.2f}")
print(f"T-statistic: {t_stat:.4f}")
print(f"P-value: {p_value:.6f}")

if p_value < 0.05:
    print(" SIGNIFICANT: Weekend and weekday revenues are statistically different!")
else:
    print(" NOT SIGNIFICANT: No statistical difference between weekend and weekday revenues")

#  T-test 2 - Does rush hour affect fare per km?
print("\n" + "="*50)
print("TEST 2: Rush Hour vs Non-Rush Hour Pricing")
print("="*50)

rush_hour_fares = trip_df[trip_df['is_rush_hour'] == 1]['fare_per_km']
non_rush_fares = trip_df[trip_df['is_rush_hour'] == 0]['fare_per_km']

t_stat2, p_value2 = ttest_ind(rush_hour_fares, non_rush_fares)

print(f"Rush hour average fare per km: ${rush_hour_fares.mean():.2f}")
print(f"Non-rush hour average fare per km: ${non_rush_fares.mean():.2f}")
print(f"T-statistic: {t_stat2:.4f}")
print(f"P-value: {p_value2:.6f}")

if p_value2 < 0.05:
        print(" SIGNIFICANT: Rush hour pricing is statistically different!")
else:
        print(" NOT SIGNIFICANT: No statistical difference in rush hour pricing")

# Test 3: ANOVA - Do different zones have significantly different revenues?
print("\n" + "="*50)
print("TEST 3: Zone-wise Revenue Differences (ANOVA)")
print("="*50)

# get top 5 zones by trip count for cleaner analysis
top_zones = trip_df['PU_Zone'].value_counts().head(5).index.tolist()
zone_revenue_groups = []

for zone in top_zones:
    zone_data = trip_df[trip_df['PU_Zone'] == zone]['total_amount']
    zone_revenue_groups.append(zone_data)
    print(f"{zone}: Average revenue ${zone_data.mean():.2f}, Count: {len(zone_data)}")

# Perform one-way ANOVA
f_stat, p_value3 = f_oneway(*zone_revenue_groups)

print(f"\nF-statistic: {f_stat:.4f}")
print(f"P-value: {p_value3:.6f}")

if p_value3 < 0.05:
    print(" SIGNIFICANT: Different zones have statistically different revenues!")
    print(" Business insight: Zone-based pricing/resource allocation is justified!")
else:
    print(" NOT SIGNIFICANT: No statistical difference between zone revenues")

# Test 4: Chi-square test - Are rush hour patterns independent of day of week?
print("\n" + "="*50)
print("TEST 4: Rush Hour Independence from Day of Week")
print("="*50)

# Create contingency table
contingency_table = pd.crosstab(trip_df['pickup_day_of_week'], trip_df['is_rush_hour'])
print("Contingency Table:")
print(contingency_table)

# Perform chi-square test
chi2_stat, p_value4, dof, expected = chi2_contingency(contingency_table)

print(f"\nChi-square statistic: {chi2_stat:.4f}")
print(f"P-value: {p_value4:.6f}")
print(f"Degrees of freedom: {dof}")

if p_value4 < 0.05:
      print(" SIGNIFICANT: Rush hour patterns depend on day of week!")
      print(" Business insight: Different days have different rush hour intensities!")
else:
      print(" NOT SIGNIFICANT: Rush hour patterns are independent of day of week")

# Summary of all tests
print("\n" + "="*60)
print("SUMMARY OF STATISTICAL TESTS")
print("="*60)
tests_summary = [
    ("Weekend vs Weekday Revenue", p_value < 0.05, p_value),
        ("Rush Hour vs Non-Rush Pricing", p_value2 < 0.05, p_value2),
            ("Zone Revenue Differences", p_value3 < 0.05, p_value3),
                ("Rush Hour Day Independence", p_value4 < 0.05, p_value4)
                ]

for test_name, is_significant, p_val in tests_summary:
     status = "SIGNIFICANT" if is_significant else "NOT SIGNIFICANT"
     print(f"{test_name}: {status} (p={p_val:.6f})")

"""<br>ðŸ“Š STRATEGIC RECOMMENDATIONS:
Immediate Actions (High Priority):
Fix surge pricing strategy - Rush hours should command premium rates
Implement zone-based pricing - Charge more in high-revenue zones
Weekend-specific operations - Different strategies for weekends
"""

# Test 1: Check if trip volume follows Poisson distribution (common for count data)
print("="*60)
print("TEST 1: Distribution Analysis for Trip Volume")
print("="*60)

# Create hourly trip counts
hourly_trips = trip_df.groupby(['pickup_hour']).size().values

print(f"Trip volume statistics:")
print(f"Mean: {hourly_trips.mean():.2f}")
print(f"Variance: {hourly_trips.var():.2f}")
print(f"Standard deviation: {hourly_trips.std():.2f}")
print(f"Variance/Mean ratio: {hourly_trips.var()/hourly_trips.mean():.2f}")
print("(For Poisson: variance should â‰ˆ mean, ratio â‰ˆ 1.0)")

# Test for Poisson distribution
# For Poisson test, we'll use Kolmogorov-Smirnov test
estimated_lambda = hourly_trips.mean()
ks_stat, p_value_poisson = stats.kstest(hourly_trips,
                                        lambda x: poisson.cdf(x, estimated_lambda))

print(f"\nPoisson Distribution Test:")
print(f"Estimated Î» (lambda): {estimated_lambda:.2f}")
print(f"KS statistic: {ks_stat:.4f}")
print(f"P-value: {p_value_poisson:.6f}")

if p_value_poisson > 0.05:
  print(" FOLLOWS POISSON: Use Poisson regression for demand forecasting!")
else:
  print(" NOT POISSON: Consider Negative Binomial or other models")

# Test 2: Normality test for fare_amount
print("\n" + "="*60)
print("TEST 2: Normality Test for Fare Amount")
print("="*60)

# Clean data (remove extreme outliers for cleaner test)
fare_clean = trip_df[(trip_df['fare_amount'] > 0) & (trip_df['fare_amount'] < 100)]['fare_amount']

print(f"Fare amount statistics (cleaned):")
print(f"Mean: ${fare_clean.mean():.2f}")
print(f"Median: ${fare_clean.median():.2f}")
print(f"Std: ${fare_clean.std():.2f}")
print(f"Skewness: {fare_clean.skew():.3f}")
print(f"Kurtosis: {fare_clean.kurtosis():.3f}")

# Shapiro-Wilk test (sample size limit)
if len(fare_clean) > 5000:
    fare_sample = fare_clean.sample(5000, random_state=42)
else:
    fare_sample = fare_clean

shapiro_stat, shapiro_p = shapiro(fare_sample)
print(f"\nShapiro-Wilk Test (sample of {len(fare_sample)} trips):")
print(f"Statistic: {shapiro_stat:.4f}")
print(f"P-value: {shapiro_p:.6f}")

# Jarque-Bera test (better for large samples)
jb_stat, jb_p = jarque_bera(fare_clean)
print(f"\nJarque-Bera Test:")
print(f"Statistic: {jb_stat:.4f}")
print(f"P-value: {jb_p:.6f}")

if shapiro_p > 0.05 and jb_p > 0.05:
    print(" NORMAL DISTRIBUTION: Use linear regression, t-tests valid")
else:
    print(" NOT NORMAL: Consider log-transformation or non-parametric methods")

    # Visualize normality
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.hist(fare_clean, bins=50, alpha=0.7, color='Gold', edgecolor='black')
    plt.title('Fare Amount Distribution')
    plt.xlabel('Fare Amount ($)')
    plt.ylabel('Frequency')

    plt.subplot(1, 2, 2)
    stats.probplot(fare_clean, dist="norm", plot=plt)
    plt.title('Q-Q Plot (Normal Distribution)')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Test 3: Outlier Detection
print("\n" + "="*60)
print("TEST 3: Outlier Detection Analysis")
print("="*60)

#  IQR Method for fare_amount
Q1 = trip_df['fare_amount'].quantile(0.25)
Q3 = trip_df['fare_amount'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

iqr_outliers = trip_df[(trip_df['fare_amount'] < lower_bound) | (trip_df['fare_amount'] > upper_bound)]

print(f"IQR Method - Fare Amount:")
print(f"Q1: ${Q1:.2f}, Q3: ${Q3:.2f}, IQR: ${IQR:.2f}")
print(f"Outlier bounds: ${lower_bound:.2f} to ${upper_bound:.2f}")
print(f"Number of outliers: {len(iqr_outliers):,} ({len(iqr_outliers)/len(trip_df)*100:.1f}% of data)")

#  Z-score method for trip_distance
z_scores = np.abs(stats.zscore(trip_df['trip_distance']))
z_outliers = trip_df[z_scores > 3]

print(f"\nZ-score Method - Trip Distance:")
print(f"Outliers (|z| > 3): {len(z_outliers):,} ({len(z_outliers)/len(trip_df)*100:.1f}% of data)")

#  Isolation Forest for multivariate outliers
print(f"\nIsolation Forest - Multivariate Outliers:")
# Select features for outlier detection
outlier_features = ['trip_distance', 'fare_amount', 'trip_duration_minutes', 'tip_amount']
outlier_data = trip_df[outlier_features].dropna()

# Fit Isolation Forest
iso_forest = IsolationForest(contamination=0.05, random_state=42)
outlier_predictions = iso_forest.fit_predict(outlier_data)

multivariate_outliers = outlier_data[outlier_predictions == -1]
print(f"Multivariate outliers detected: {len(multivariate_outliers):,} ({len(multivariate_outliers)/len(outlier_data)*100:.1f}% of data)")

# Analyze outlier characteristics
print(f"\nOutlier Characteristics (IQR method):")
if len(iqr_outliers) > 0:
    print(f"Average fare of outliers: ${iqr_outliers['fare_amount'].mean():.2f}")
    print(f"Max fare outlier: ${iqr_outliers['fare_amount'].max():.2f}")
    print(f"Most common outlier zones:")
    print(iqr_outliers['PU_Zone'].value_counts().head(3))

print("\n" + "="*60)
print("DISTRIBUTION ANALYSIS SUMMARY")
print("="*60)

distribution_summary = [
    ("Trip Volume Distribution", "Poisson" if p_value_poisson > 0.05 else "Non-Poisson", p_value_poisson),
        ("Fare Amount Normality", "Normal" if jb_p > 0.05 else "Non-Normal", jb_p),
            ("Outlier Percentage (IQR)", f"{len(iqr_outliers)/len(trip_df)*100:.1f}%", None),
                ("Outlier Percentage (Multivariate)", f"{len(multivariate_outliers)/len(outlier_data)*100:.1f}%", None)
                ]

for analysis, result, p_val in distribution_summary:
       if p_val is not None:
           print(f"{analysis}: {result} (p={p_val:.6f})")
       else:
           print(f"{analysis}: {result}")

"""# **Revenue Prediction**

### Data Preparation
"""

# Based on your VIF analysis:
features_to_remove_for_modeling = ['total_amount', 'fare_amount']
features_to_keep_for_modeling = [
    'trip_distance', 'tip_amount', 'fare_per_km', 'tip_percent',
    'trip_duration_minutes', 'pickup_hour', 'pickup_day_of_week',
    'passenger_count','trip_route'
    ]

df = trip_df.copy()
original_shape = len(df)

# Remove only impossible values (not outliers)
df = df[
    (df['fare_amount'] > 0) &           # Positive fare
    (df['trip_distance'] > 0) &         # Positive distance
    (df['total_amount'] > 0) &          # Positive total
    (df['tip_amount'] >= 0) &           # Non-negative tips
    (df['passenger_count'] > 0) &       # At least 1 passenger
    (df['trip_duration_minutes'] > 0)   # Positive duration
    ]

# create target variable
df['revenue_per_minute'] = df['total_amount'] / df['trip_duration_minutes']
target_variable = 'revenue_per_minute'

df = df.replace([np.inf, -np.inf], np.nan)
df = df.dropna(subset=['revenue_per_minute'])

print(df[target_variable].describe())

df_model = df[features_to_keep_for_modeling + [target_variable]].dropna()

# Show outlier statistics we're keeping
print(f"\nOutlier statistics we're KEEPING:")
print(f"Revenue per minute range: ${df_model[target_variable].min():.2f} to ${df_model[target_variable].max():.2f}")
print(f"Trip distance range: {df_model['trip_distance'].min():.1f} to {df_model['trip_distance'].max():.1f} miles")
print(f"Tip amount range: ${df_model['tip_amount'].min():.2f} to ${df_model['tip_amount'].max():.2f}")

print(f"\nFeature correlation with target:")
correlations = df_model[features_to_keep_for_modeling].corrwith(df_model[target_variable])
print(correlations.sort_values(ascending=False))

"""###Feature Engineering

"""

# trip efficiency (distance per minute)
df_model['distance_per_minute'] = df_model['trip_distance'] / df_model['trip_duration_minutes']

# high-value trip indicator
df_model['is_high_value'] = (df_model['tip_amount'] > df_model['tip_amount'].quantile(0.75)).astype(int)

# busyday Like Friday & Saturday
df_model['is_busyday'] = (df_model['pickup_day_of_week'].isin([4, 5])).astype(int)

# peak hours (morning and evening rush)
df_model['is_peak_hour'] = (
    ((df_model['pickup_hour'] >= 7) & (df_model['pickup_hour'] <= 9)) |
    ((df_model['pickup_hour'] >= 17) & (df_model['pickup_hour'] <= 19))).astype(int)

features_final = [
    'trip_distance', 'tip_amount', 'fare_per_km', 'tip_percent',
    'trip_duration_minutes', 'pickup_hour', 'pickup_day_of_week',
    'passenger_count' ,'distance_per_minute', 'is_high_value', 'is_busyday', 'is_peak_hour']

route_target_mean = df.groupby('trip_route')['revenue_per_minute'].mean().to_dict()
df['trip_route_encoded'] = df['trip_route'].map(route_target_mean)


df[['trip_route', 'trip_route_encoded']].head()

"""## Data Splitting"""

X = df_model[features_final]
y = df_model['revenue_per_minute']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=None)

train_correlations = X_train.corrwith(y_train)
print(train_correlations.sort_values(ascending=False))

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.preprocessing import StandardScaler
import time

models = {
      'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
      'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
      'XGBoost': XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)
       }

results = {}

for name, model in models.items():
    print(f"\n Training {name}...")
    start_time = time.time()

    model.fit(X_train, y_train)
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)

    # evaluation metrics
    train_mae = mean_absolute_error(y_train, y_pred_train)
    test_mae = mean_absolute_error(y_test, y_pred_test)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)

    training_time = time.time() - start_time

    results[name] = {
        'model': model,
        'train_mae': train_mae,
        'test_mae': test_mae,
        'train_rmse': train_rmse,
        'test_rmse': test_rmse,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'training_time': training_time,
        'predictions': y_pred_test
    }

    print(f" {name} completed in {training_time:.2f} seconds")
    print(f"   Train MAE: ${train_mae:.3f} | Test MAE: ${test_mae:.3f}")
    print(f"   Train RÂ²: {train_r2:.3f} | Test RÂ²: {test_r2:.3f}")

comparison_df = pd.DataFrame({
    'Model': list(results.keys()),
    'Test MAE ($)': [f"${results[name]['test_mae']:.3f}" for name in results.keys()],
    'Test RÂ²': [f"{results[name]['test_r2']:.3f}" for name in results.keys()],
    'Test RMSE ($)': [f"${results[name]['test_rmse']:.3f}" for name in results.keys()],
    'Training Time (s)': [f"{results[name]['training_time']:.2f}" for name in results.keys()]
})

print(comparison_df.to_string(index=False))

# Find best model (lowest test MAE)
best_model_name = min(results.keys(), key=lambda x: results[x]['test_mae'])
best_model = results[best_model_name]['model']

print(f"\n BEST MODEL: {best_model_name}")
print(f"   Best Test MAE: ${results[best_model_name]['test_mae']:.3f}")
print(f"   Best Test RÂ²: {results[best_model_name]['test_r2']:.3f}")

"""# **MODEL 2: Hourly Zone-Based Taxi Demand Forecasting**

### Feature Engineering
"""

#Create is_busyday feature (Friday and Saturday)
trip_df['is_busyday'] = (trip_df['pickup_day_of_week'].isin([4, 5])).astype(int)

# Extract date from datetime column
trip_df['pickup_date'] = trip_df['tpep_pickup_datetime'].dt.date

demand_df = trip_df.groupby(['PU_Zone', 'pickup_date', 'pickup_hour']).agg({
    'tpep_pickup_datetime': 'count',
    'is_busyday': 'first',
    'pickup_day_of_week': 'first',
    'is_rush_hour': 'first'
}).reset_index()


demand_df.rename(columns={'tpep_pickup_datetime': 'demand'}, inplace=True)

# Create lag features

# Sort by zone and time first
demand_df = demand_df.sort_values(['PU_Zone', 'pickup_date', 'pickup_hour'])

# Add lag features for each zone
demand_df['demand_last_hour'] = demand_df.groupby('PU_Zone')['demand'].shift(1)
demand_df['demand_yesterday_same_hour'] = demand_df.groupby('PU_Zone')['demand'].shift(24)
demand_df['demand_avg_last_3hours'] = demand_df.groupby('PU_Zone')['demand'].rolling(3).mean().shift(1).values

# Fill missing values with 0
demand_df[['demand_last_hour', 'demand_yesterday_same_hour', 'demand_avg_last_3hours']] = demand_df[['demand_last_hour', 'demand_yesterday_same_hour', 'demand_avg_last_3hours']].fillna(0)

feature_columns = [
    'PU_Zone',
    'pickup_hour',
    'pickup_day_of_week',
    'is_busyday',
    'is_rush_hour',
    'demand_last_hour',
    'demand_yesterday_same_hour',
    'demand_avg_last_3hours'
]


model_df = demand_df[feature_columns + ['demand']].copy()

# check data quality
print("Data Quality Check:")
print(f"Total records: {len(model_df)}")
print(f"Records with missing values: {model_df.isnull().sum().sum()}")

print("\nBasic Statistics:")
print(model_df['demand'].describe())

print("\nFeature Summary:")
print(f"Unique zones: {model_df['PU_Zone'].nunique()}")
print(f"Hours covered: {model_df['pickup_hour'].nunique()}")
print(f"Busy days percentage: {model_df['is_busyday'].mean()*100:.1f}%")
print(f"Rush hours percentage: {model_df['is_rush_hour'].mean()*100:.1f}%")

clean_df = model_df.dropna()


X = clean_df[feature_columns].copy()
y = clean_df['demand']

# Convert zone to dummy variables
X_encoded = pd.get_dummies(X, columns=['PU_Zone'])

# Quick Random Forest to see what features matter most
rf_temp = RandomForestRegressor(n_estimators=50, random_state=42)
rf_temp.fit(X_encoded, y)


importance_df = pd.DataFrame({
    'feature': X_encoded.columns,
        'importance': rf_temp.feature_importances_
        }).sort_values('importance', ascending=False)

print("Top 10 Most Important Features:")
print(importance_df.head(10))

X_train, X_test, y_train, y_test = train_test_split( X_encoded, y,  test_size=0.2, random_state=42)

"""## RandomForestRegressor Hyper Parameter Tuing"""

rf_params = {
    'n_estimators': randint(100, 200),
    'max_depth': randint(10, 30),
    'min_samples_split': randint(2, 10),
    'min_samples_leaf': randint(1, 5)
}

rf = RandomForestRegressor(random_state=42)

rf_random = RandomizedSearchCV(
    estimator=rf,
    param_distributions=rf_params,
    n_iter=20,
    cv=5,
    scoring='neg_mean_absolute_error',
    verbose=2,
    random_state=42,
    n_jobs=-1
)

rf_random.fit(X_train, y_train)

print("Best RF Params:", rf_random.best_params_)
print("Best RF CV Score (MAE):", -rf_random.best_score_)

rf_best = rf_random.best_estimator_
rf_preds = rf_best.predict(X_test)

print("RF MAE:", mean_absolute_error(y_test, rf_preds))
print("RF RMSE:", np.sqrt(mean_squared_error(y_test, rf_preds)))
print("RF R2:", r2_score(y_test, rf_preds))

"""## XGBoost Regresson HyperParameter Tuning"""

xgb_params = {
     'n_estimators': randint(100, 200),
     'max_depth': randint(3, 8),
     'learning_rate': uniform(0.05, 0.15),
     'subsample': uniform(0.7, 0.3),
     'colsample_bytree': uniform(0.7, 0.3)
 }

 xgb = XGBRegressor(objective='reg:squarederror', random_state=42)

 xgb_random = RandomizedSearchCV(
     estimator=xgb,
     param_distributions=xgb_params,
     n_iter=20,
     cv=5,
     scoring='neg_mean_absolute_error',
     verbose=2,
     random_state=42,
     n_jobs=-1
 )

 xgb_random.fit(X_train, y_train)
 print("Best XGB Params:", xgb_random.best_params_)
 print("Best XGB CV Score (MAE):", -xgb_random.best_score_)

 xgb_best = xgb_random.best_estimator_
 xgb_preds = xgb_best.predict(X_test)
 print("XGB MAE:", mean_absolute_error(y_test, xgb_preds))
 print("XGB RMSE:", np.sqrt(mean_squared_error(y_test, xgb_preds)))
 print("XGB R2:", r2_score(y_test, xgb_preds))

"""## Gradient Boosting Regressor HyperParameter Tuning"""

gbr_params = {
    'n_estimators': randint(100, 200),
    'learning_rate': uniform(0.05, 0.15),
    'max_depth': randint(3, 8),
    'min_samples_split': randint(2, 10),
    'min_samples_leaf': randint(1, 5)
}

gbr = GradientBoostingRegressor(random_state=42)

gbr_random = RandomizedSearchCV(
    estimator=gbr,
    param_distributions=gbr_params,
    n_iter=20,
    cv=5,
    scoring='neg_mean_absolute_error',
    verbose=2,
    random_state=42,
    n_jobs=-1
)

gbr_random.fit(X_train, y_train)

print("Best GBR Params:", gbr_random.best_params_)
print("Best GBR CV Score (MAE):", -gbr_random.best_score_)

gbr_best = gbr_random.best_estimator_
gbr_preds = gbr_best.predict(X_test)

print("GBR MAE:", mean_absolute_error(y_test, gbr_preds))
print("GBR RMSE:", np.sqrt(mean_squared_error(y_test, gbr_preds)))
print("GBR R2:", r2_score(y_test, gbr_preds))

# selecting the best model
best_model = rf_best

"""## Prediction Function"""

def predict_demand(zone, hour, day_of_week, is_busyday, is_rush_hour,
                   demand_last_hour=0, demand_yesterday=0, demand_avg_3hr=0):

    # Create input dataframe
    input_data = pd.DataFrame({
        'pickup_hour': [hour],
        'pickup_day_of_week': [day_of_week],
        'is_busyday': [is_busyday],
        'is_rush_hour': [is_rush_hour],
        'demand_last_hour': [demand_last_hour],
        'demand_yesterday_same_hour': [demand_yesterday],
        'demand_avg_last_3hours': [demand_avg_3hr]
    })

    for col in X_encoded.columns:
        if col.startswith('PU_Zone_'):
            if col == f'PU_Zone_{zone}':
                input_data[col] = 1
            else:
                input_data[col] = 0
        elif col not in input_data.columns:
            input_data[col] = 0


    input_data = input_data[X_encoded.columns]

    prediction = best_model.predict(input_data)[0]

    return max(0, round(prediction))

print("Prediction pipeline created!")

# testing ex1
pred1 = predict_demand(
    zone='Manhattan',
    hour=17,
    day_of_week=4,
    is_busyday=1,
    is_rush_hour=1,
    demand_last_hour=25,
    demand_yesterday=30,
    demand_avg_3hr=22
)
print(f"Example  - Manhattan, 5PM Friday (rush): {pred1} trips predicted")

# testig ex 2
pred2 = predict_demand(
    zone='Queens',
    hour=2,
    day_of_week=1,
    is_busyday=0,
    is_rush_hour=0,
    demand_last_hour=3,
    demand_yesterday=5,
    demand_avg_3hr=4
)
print(f"Example  - Queens, 2AM Tuesday (quiet): {pred2} trips predicted")